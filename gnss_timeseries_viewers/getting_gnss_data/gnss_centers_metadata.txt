May 31, 2018
Instructions for periodically updating the GPS files on your local machine
For doing this update in 2020, I created a script (update_data_holdings.sh)

PBO DOWNLOAD: Velocities, Offsets, and Data
pbo_gps_dir: 
	source: ftp://data-out.unavco.org/pub/products/position/
	The command to download into PBO_stations is 
		wget --recursive --no-parent --no-directories --accept "*.pbo.final_igs08.pos, *.pbo.final_nam08.pos, *.cwu.final_nam08.pos, *.nmt.final_nam08.pos" ftp://data-out.unavco.org/pub/products/position
	Takes about 2 hours, about 5 GB. 
	NOTE: On October 1 2018, the GAGE processing center stopped funding both CWU and NMT.  They are only supporting 
	CWU from this point forward. As a result, NMT will only go until September 15, 2018. 
	All new downloads only get CWU data, since the others are static.
pbo_velocities: monthly-updated velocity fields
	source: ftp://data-out.unavco.org/pub/products/velocity/
	Download your preferred velocity field
pbo_earthquakes_dir: Event files for PBO earthquakes
	source: ftp://data-out.unavco.org/pub/products/event/
	Download all *coseis_kalts.evt and put them into this directory
pbo_offsets_dir: PBO offsets from antenna changes, etc.
	source: ftp://data-out.unavco.org/pub/products/offset/
	Download single file and put it into this directory
	"wget ftp://data-out.unavco.org/pub/products/offset/*nam14.off"



UNR DOWNLOAD: Velocities, Offsets, Data Holdings, and Data
unr_coords_file: 
	Download coords file and place it into this file. Update the pointer in your config file. 
	source: http://geodesy.unr.edu/NGLStationPages/DataHoldings.txt
unr_offsets_dir: Directory for UNR offsets
	Download steps file and place it into this directory
	source: http://geodesy.unr.edu/NGLStationPages/steps.txt
	I also saved their website text as a readme in 2018. 
unr_velocities:
	Manually download velocity file and place into this directory. Update datestamp, place #URL in first line. 
	source: http://geodesy.unr.edu/velocities (not going to direct table URL because HTML formatting was weird)
	I manually download midas.IGS14.txt and midas.NA.txt (the ones that are actively updated)
unr_gps_dir: Time series data from UNR
	download all .tenv3 files in your region of interest and put them into this directory. 
	source: http://geodesy.unr.edu/index.php
	Example: ```python /Users/kmaterna/Documents/B_Research/Github_Tools/GNSS_TimeSeries_Viewers/getting_gnss_data/get_unr_time_series.py ../Metadata/UNR_coords_sept2022.txt ```
	Just downloading NA and IGS14, because NA12 processing is static after 2019. Takes about 1 hour for WUS. 


USGS DOWNLOAD:
Last downloaded September 17, 2022
Takes about 2 hours. Run using the 'research' conda environment. 
Run "get_usgs_data.py" script to download velocity, time series, and offset files from USGS. 


HYDROLOGICAL MODELS DOWNLOAD:
Takes about 15 minutes for each product
gldas_dir: Put GLDAS models in this directory
	"wget --recursive --no-parent --no-directories --accept *.hyd ftp://data-out.unavco.org/pub/products/hydro/gldas2"
nldas_dir: Put NLDAS models in this directory
	"wget --recursive --no-parent --no-directories --accept *.hyd ftp://data-out.unavco.org/pub/products/hydro/nldas2"
noah_dir: Put NOAH models in this directory
	"wget --recursive --no-parent --no-directories --accept *.hyd ftp://data-out.unavco.org/pub/products/hydro/noah025"
lsdm_dir: 
	There's a nice extractor script located at http://rz-vm115.gfz-potsdam.de:8080/repository/entry/show?entryid=362f8705-4b87-48d1-9d86-2cfd1a2b6ac9
	Download that script and then use my utility get_lsdm.py to extract all your stations. Takes a while. 

